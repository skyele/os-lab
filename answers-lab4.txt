Exercise 1. Implement mmio_map_region in kern/pmap.c . To see how this is used, look at the beginning of lapic_init in kern/lapic.c . You'll have to do the next
exercise, too, before the tests for mmio_map_region will run.
	通过阅读lapic_init函数的代码：
lapic = mmio_map_region(lapicaddr, 4096);
	也就是在MMIO的虚拟地址之上，为从物理地址lapicaddr开始之后的4096个字节，分配一片连续的虚拟地址。
	通过调用boot_map_region()函数，可以将物理地址pa之后size个bytes，映射到从初始值为MMIO的base虚拟地址空间。其中要主义的是，我们需要注意的是，由于这是lapic的io memory，与常规的memory不同。不同之处在于，对于常规的memory，我们默认cache中的数据是比memory新的，因此，cpu总是先在cache中读取缓存的memory的数据。而对于lapic这种mmio映射的memory，memory中的数据是会比cache中更新的，因为它的数据直接来自硬件。因此我们要显示的告诉cpu，不能从cache中读数据，因此需要置上相应的位：PTE_PCD | PTE_PWT. 

Exercise 2. Read boot_aps() and mp_main() in kern/init.c , and the assembly
code in kern/mpentry.S . Make sure you understand the control flow transfer during the bootstrap of APs. Then modify your implementation of page_init() in kern/pmap.c to avoid adding the page at MPENTRY_PADDR to the free list, so that we can safely copy and run AP bootstrap code at that physical address. Your code should pass the updated check_page_free_list() test (but might fail the updated check_kern_pgdir() test, which we will fix soon).
	只要在page_init()中对于pages的初始化中，加入对于地址为MPENTRY_PADDR时，不要把这片地址加入page_free_list即可。代码如下：
else if(i == MPENTRY_PADDR/PGSIZE){
	pages[i].pp_ref = 1;
	pages[i].pp_link = NULL;
}

Question
1. Compare kern/mpentry.S side by side with boot/boot.S . Bearing in mind that
kern/mpentry.S is compiled and linked to run above KERNBASE just like
everything else in the kernel, what is the purpose of macro MPBOOTPHYS ? Why is it necessary in kern/mpentry.S but not in boot/boot.S ? In other words, what could go wrong if it were omitted in kern/mpentry.S ?
Hint: recall the differences between the link address and the load address that we have discussed in Lab 1.
	MPBOOTPHYS用于计算symbol的绝对地址。因为链接时地址并不一定等于加载时地址，因此不能依靠链接器来计算这个地址。

Exercise 3. Modify mem_init_mp() (in kern/pmap.c ) to map per-CPU stacks starting at KSTACKTOP , as shown in inc/memlayout.h . The size of each stack is KSTKSIZE
	为了支持symmetric multiple processor，我们需要对不同的cpu分配不同的栈。阅读memorylayout.h中的代码，在KSTACKTOP的下面已经为不同的cpu预留了栈空间，因此，在mem_init_mp中，直接调用boot_map_region为每一个stack分配一个物理页即可。代码如下：
for(int i = 0; i < NCPU; i++){
	boot_map_region(kern_pgdir, KSTACKTOP - i * (KSTKSIZE + KSTKGAP) - KSTKSIZE,
	KSTKSIZE, PADDR((percpu_kstacks[i])), PTE_W);
}

Exercise 4. The code in trap_init_percpu() ( kern/trap.c ) initializes the TSS and TSS descriptor for the BSP. It worked in Lab 3, but is incorrect when running on other CPUs. Change the code so that it can work on all CPUs. (Note: your new code should not use the global ts variable any more.)
	lab3中的trap_init_percpu，是只要服务一个cpu即可。因此ts和stack只要初始化cpu0的即可。但是到SMP中，多核的情景下，这种硬编码方式不再合适，我们需要通过thiscpu->cpu_ts来指代当前cpu的ts。同时tss也需要修改成为对应每一个cpu的tss。也即将GD_TSS0替换为GD_TSS0 + (cpunum() << 3)即可。在替换过后，代码如下：
int i = cpunum();
(thiscpu->cpu_ts).ts_esp0 = KSTACKTOP - i * (KSTKSIZE + KSTKGAP);
(thiscpu->cpu_ts).ts_ss0 = GD_KD;
(thiscpu->cpu_ts).ts_iomb = sizeof(struct Taskstate);
int GD_TSSi = GD_TSS0 + (i << 3);
gdt[GD_TSSi >> 3] = SEG16(STS_T32A, (uint32_t) (&(thiscpu->cpu_ts)),
	sizeof(struct Taskstate) - 1, 0);
gdt[GD_TSSi >> 3].sd_s = 0;
ltr(GD_TSSi);
lidt(&idt_pd);

Exercise 5. Apply the big kernel lock as described above, by calling lock_kernel()
and unlock_kernel() at the proper locations.
	在多核情形下，由于内核被映射为同一物理地址，因此我们需要用lock来保护kernel的执行。系统的spinlock实现为我们提供了lock_kernel()和unlock_kernel的函数实现，供我们拿锁和放锁。那么具体加锁和放锁的位置在哪呢？网站已经给出了具体位置，他们分别在
1.i386_init(),需要在BSP唤醒其他cpu前拿锁
2.需要在mp_main中，初始化ap之后拿锁
3.需要在trap()函数中，确定是在用户态触发的trap的情况下拿锁。
4.需要在env_run()函数中，在切换到用户态之前放锁。

Question 2. It seems that using the big kernel lock guarantees that only one CPU can run the kernel code at a time. Why do we still need separate kernel stacks for each CPU?
Describe a scenario in which using a shared kernel stack will go wrong, even with the protection of the big kernel lock
1.	如果需要kernel变成可多线程运行或者可以被中断的，那么我们需要不同的栈来保存当前cpu的运行状态。
2.	如果需要支持线程在核间的迁移，这样也需要多个cpustack。

Exercise 6. Implement round-robin scheduling in sched_yield() as described above. Don't forget to modify syscall() to dispatch sys_yield() .
Make sure to invoke sched_yield() in mp_main .
Modify kern/init.c to create three (or more!) environments that all run the program user/yield.c .
Run make qemu. You should see the environments switch back and forth between each
other five times before terminating, like below.
Test also with several CPUS: make qemu CPUS=2.
...
Hello, I am environment 00001000.
Hello, I am environment 00001001.
Hello, I am environment 00001002.
Back in environment 00001000, iteration 0.
Back in environment 00001001, iteration 0.
Back in environment 00001002, iteration 0.
Back in environment 00001000, iteration 1.
Back in environment 00001001, iteration 1.
Back in environment 00001002, iteration 1.
...
After the yield programs exit, there will be no runnable environment in the system, the
scheduler should invoke the JOS kernel monitor. If any of this does not happen, then fix your code before proceeding.
	如何实现round-robin的调度策略呢？那就是在每次调用env_run的时候，找到下一个status为ENV_RUNNABLE的env去跑。具体代码如下：			int i;
if(curenv){
	envid_t cur_tone = ENVX(curenv->env_id);
	for(i = ENVX(cur_tone + 1); i != cur_tone; i = ENVX(i+1)){
		if(envs[i].env_status == ENV_RUNNABLE){
			env_run(&envs[i]);
		}
	}
	if(curenv->env_status == ENV_RUNNING)
		env_run(curenv);
}
else{
	for(i = 0 ; i < NENV; i++)
  		if(envs[i].env_status == ENV_RUNNABLE) {
	 		env_run(&envs[i]);
	  	}
}
sched_halt();
	根据提示，我们首先需要判断，当前是否已经有environment在运行。如果没有，则遍历整个envs数组，找到status为ENV_RUNNABLE的environment来执行就行。如果有，那么就从这个env开始往后面查询即可。
但是在做round-robin的时候，我发现总出现无限重启的情况，定位后发现，是原来的boot_map_region_large引入的问题。直接将boot_map_region_large替换成boot_map_region即可。

Question 3. In your implementation of env_run() you should have called lcr3() . Before and after the call to lcr3() , your code makes references (at least it should) to the variable e , the argument to env_run . Upon loading the %cr3 register, the addressing context used by the MMU is instantly changed. But a virtual address (namely e ) has meaning relative to a given address context--the address context specifies the physical address to which the virtual address maps. Why can the pointer e be dereferenced both before and after the addressing switch?
	因为e的虚拟地址在内核区域，而所有env的内核区域映射都相同，因此在lcr3之后解引用也是同样的值。

4. Whenever the kernel switches from one environment to another, it must ensure the old environment's registers are saved so they can be restored properly later. Why? Where does this happen?
	因为在我们的os实现里，env是需要不断被调度的，因此，在调度到它的时候，需要接着上一次被打断执行的地方继续执行，因此被打断的时候需要保存当前状态，因此需要save old env’s registers.
	当一个env从用户态到内核态的时候，它的register被保存在trapframe的数据结构中。这个逻辑是在_alltraps中被实现的。而当env需要被恢复的时候，在切换回用户态的时候，我们调用env_pop_tf函数来恢复寄存器。

Exercise 7. Implement the system calls described above in kern/syscall.c and make
sure syscall() calls them. You will need to use various functions in kern/pmap.c
and kern/env.c , particularly envid2env() . For now, whenever you call
envid2env() , pass 1 in the checkperm parameter. Be sure you check for any invalid
system call arguments, returning -E_INVAL in that case. Test your JOS kernel with
user/dumbfork and make sure it works before proceeding.
	现在需要做的是实现系统调用函数。
	首先要写的是sys_exofork函数。该函数的作用是：使用env_alloc来获得一个新的environment。由于是fork调用的，那么new environment应该和旧的env有同样的寄存器堆。唯一不同的是fork会返回两次，那么返回值分别是fork产生的env的env_id,而在sys_exofork中，我们需要将env_tf.tf_regs.reg_eax赋值成0，只有这样才能fork的两次返回，结果不同。
	接下来是sys_env_set_status函数。这个函数顾名思义，意思其实就是通过系统调用，将某i个env的status赋值为sys_env_set_status函数的参数status。
sys_page_alloc系统调用。首先通过page_alloc分配一个物理页，然后使用page_insert函数，将va映射到对应的page，其权限为perm。
	sys_page_map函数的语义是将位于源env的srcva地址处的物理页映射到目的env的dstva地址处。因此，我们首先需要使用page_lookup函数来找到srcva对应的物理页，然后通过调用page_insert函数将dstva映射到对应物理页。
sys_page_unmap函数语义是解除envid对应虚拟地址va与一物理页的映射。关键是使用page_remove来解除绑定。这里代码如下：

struct Env* env;
int ret = envid2env(envid, &env, 1);
if(ret < 0)
	return ret;
page_remove(env->env_pgdir, va);
	return 0;

	需要注意的是，在实现这些系统调用时，需要频繁的使用envid2env()函数，对于此函数的checkperm，在实现的过程中都要置为1.

Exercise 8. Implement the sys_env_set_pgfault_upcall system call. Be sure to
enable permission checking when looking up the environment ID of the target
environment, since this is a "dangerous" system call.
	sys_env_set_pgfault_upcall系统调用的实现，由于其中涉及到使用envid2env函数，我们需要关注checkperm参数。这里我们需要将其置为1。然后将func赋值给对应env的env_pgfault_upcall字段即可。代码如下：
struct Env* e;
int ret = envid2env(envid, &e, 1);
if(ret < 0){
	return ret;
}
e->env_pgfault_upcall = func;
return 0;

Exercise 9. Implement the code in page_fault_handler in kern/trap.c required to dispatch page faults to the user-mode handler. Be sure to take appropriate precautions when writing into the exception stack. (What happens if the user environment runs out of pace on the exception stack?)
	首先，结合文档，我们了解到这个page_fault_handler是可以被循环调用的，因此区分是嵌套调用，还是非嵌套调用。由于UXSTACKTOP的异常栈只有一个page，因此如果是嵌套调用的，那么当前esp与UXSTACKTOP的偏移应该小于一个page size。由此即可区分，是否是递归调用。
	然后，我们判断，如果pagefault是用户态产生的。将当前的tf赋值给对应utf，然后将eip设置为env_pgfault_upcall，由此执行对应的handler代码。
具体代码如下：
if(curenv->env_pgfault_upcall){
	struct UTrapframe* utf;
	if((uint32_t)(UXSTACKTOP - tf->tf_esp) < PGSIZE)
		utf = (struct UTrapframe *)(tf->tf_esp - sizeof(void *) - sizeof(struct UTrapframe));
	else
		utf = (struct UTrapframe *)(UXSTACKTOP - sizeof(struct UTrapframe));		
	user_mem_assert(curenv, (void *)utf, sizeof(struct UTrapframe), PTE_W);

	utf->utf_fault_va = fault_va;
	utf->utf_err = tf->tf_err;
	utf->utf_regs = tf->tf_regs;
	utf->utf_eip = tf->tf_eip;
	utf->utf_eflags = tf->tf_eflags;
	utf->utf_esp = tf->tf_esp;
	curenv->env_tf.tf_eip = (uintptr_t)curenv->env_pgfault_upcall;
	curenv->env_tf.tf_esp = (uintptr_t)utf;
	env_run(curenv);
}

Exercise 10. Implement the _pgfault_upcall routine in lib/pfentry.S . The
interesting part is returning to the original point in the user code that caused the page
fault. You'll return directly there, without going back through the kernel. The hard part is
simultaneously switching stacks and re-loading the EIP.
	这里要实现一个汇编。语义是在内核处理完page fault系统调用后，返回用户态之前的准备工作。根据exercise9我们接下来需要执行的是page_fault_handler的代码，之前也在栈上构造了一个utf数据结构，现在需要利用它返回到用户态的page_fault_handler代码逻辑中。
	根据提示，我们首先要把eip压入到utf保存的esp中。操作如下：
movl 0x28(%esp), %ebx 
movl 0x30(%esp), %eax 
subl $0x4, %eax 
movl %ebx, (%eax)
movl %eax, 0x30(%esp)
	0x28对应的是utf中eip的位置，0x30对应的是utf中esp的位置，subl 	0x4, %eax为填入eip做准备。movl %eax, 0x30(%esp)则是更新utf中esp的值。
	根据注释，接下来我们需要restore trap-time registers，由于有utf_fault_va和utf_err，需要先addl $0x8, %esp跳过他们，然后在popal，将寄存器回复。
	这里使用addl $0x4, %esp 跳过eip，然后popfl恢复eflags的值。
	然后当前指向是utf的esp字段，直接popl %esp来执行换栈操作，由于之前的eip已经压入栈，调用ret即可修改eip，执行handler逻辑。完整代码如下：
pushl %esp			
movl _pgfault_handler, %eax
call *%eax
addl $4, %esp			// pop function argument
	
movl 0x28(%esp), %ebx 
movl 0x30(%esp), %eax 
subl $0x4, %eax 
movl %ebx, (%eax)
movl %eax, 0x30(%esp)
addl $0x8, %esp 
popal

addl $0x4, %esp
popfl
	
popl %esp
ret

Exercise 11. Finish set_pgfault_handler() in lib/pgfault.c .
	根据注释，我们知道，当第一次调用此函数时，即_pgfault_handler==0时，需要注册一个handler，并且分配一个异常处理栈，并告诉内核调用exercise10中实现的汇编。
	接下来是分配异常处理栈，首先调用sys_page_alloc，在UXSTACKTOP-PGSIZE的地方分配一个PGSIZE的页作为异常栈。
	然后调用sys_env_set_pgfault_upcall，来告诉内核，在pagefault异常发生时，调用_pgfault_upcall即练习9实现的汇编代码。

Exercise 12. Implement fork , duppage and pgfault in lib/fork.c .
Test your code with the forktree program. It should produce the following messages, with interspersed 'new env', 'free env', and 'exiting gracefully' messages. The messages may not appear in this order, and the environment IDs may be different.
	Fork的逻辑是：
	父进程调用set_pgfault_handler将pgfault作为page fault触发时的handler。
然后通过调用sys_exofork创建子进程。
	通过识别sys_exofork的返回值，来分辨当前是运行在子进程还是父进程。
如果返回值是0，说明在子进程，修改对应的thisenv指针即可返回。
	如果非0，说明当前处于父进程，将UTEXT到USTACKTOP之间的所有page都duppage到子进程，而UXSTACKTOP不能是copy on write的，因为它不能copy，因此需要调用sys_page_alloc重新为子进程分配。同样，调用sys_env_set_pgfault_upcall为子进程设置page fault handler，调用sys_env_set_status将子进程状态设置为RUNNABLE即可。
	接着是duppage的实现。这里主要是调用sys_page_map将父进程的物理页映射拷贝给子进程。在拷贝的时候需要进行区分，如果父进程对应pte是有写权限的，那么我们需要将父子进程的pte都设置为PTE_COW,如果父进程对应pte只有读权限，那么不需要特殊处理。
	接下来是实现pgfault函数，主要逻辑是，如果发生pagefault的页的pte是COW的，那么需要将它重新分配一个新的物理页，将内容拷贝进去，再执行写操作。由此可见其主要逻辑是，先判断权限位，接着调用sys_page_alloc分配一个新的物理页。由于为了防止中间状态的暴露，我们先分配一个临时页，将数据拷贝进去。然后再调用sys_page_map修改对应addr处的物理页映射，指向PFTEMP，之后再通过sys_page_unmap释放PFTEMP。	
	主要逻辑如下：
addr = ROUNDDOWN(addr, PGSIZE);
int ret;
ret = sys_page_alloc(0, (void *)PFTEMP, PTE_P | PTE_U | PTE_W);
memcpy((void *)PFTEMP, (void *)addr, PGSIZE);
ret = sys_page_map(0, PFTEMP, 0, addr,  PTE_P | PTE_U | PTE_W);
ret = sys_page_unmap(0, (void *)PFTEMP);

Exercise 13. Modify kern/trapentry.S and kern/trap.c to initialize the appropriate entries in the IDT and provide handlers for IRQs 0 through 15. Then modify the code in env_alloc() in kern/env.c to ensure that user environments are always run with interrupts enabled. 
	Also uncomment the sti instruction in sched_halt() so that idle CPUs unmask interrupts. The processor never pushes an error code when invoking a hardware interrupt handler. You might want to re-read section 9.2 of the 80386 Reference Manual, or section 5.8 of the IA-32 Intel Architecture Software Developer's Manual, Volume 3 at this time.
	与之前的异常注册类似，再trapentry.S与trap.c中注册IDT中的处理handler。
	TRAPHANDLER_NOEC(TIMER_HANDLER	  , IRQ_OFFSET + IRQ_TIMER)
	TRAPHANDLER_NOEC(KBD_HANDLER	  , IRQ_OFFSET + IRQ_KBD)
	TRAPHANDLER_NOEC(SECOND_HANDLER	  , IRQ_OFFSET + 2)
	TRAPHANDLER_NOEC(THIRD_HANDLER	  , IRQ_OFFSET + 3)
	TRAPHANDLER_NOEC(SERIAL_HANDLER	  , IRQ_OFFSET + IRQ_SERIAL)
	TRAPHANDLER_NOEC(FIFTH_HANDLER	  , IRQ_OFFSET + 5)
	TRAPHANDLER_NOEC(SIXTH_HANDLER	  , IRQ_OFFSET + 6)
	TRAPHANDLER_NOEC(SPURIOUS_HANDLER , IRQ_OFFSET + IRQ_SPURIOUS)
	TRAPHANDLER_NOEC(EIGHTH_HANDLER	  , IRQ_OFFSET + 8)
	TRAPHANDLER_NOEC(NINTH_HANDLER	  , IRQ_OFFSET + 9)
	TRAPHANDLER_NOEC(TENTH_HANDLER	  , IRQ_OFFSET + 10)
	TRAPHANDLER_NOEC(ELEVEN_HANDLER	  , IRQ_OFFSET + 11)
	TRAPHANDLER_NOEC(TWELVE_HANDLER	  , IRQ_OFFSET + 12)
	TRAPHANDLER_NOEC(THIRTEEN_HANDLER , IRQ_OFFSET + 13)
	TRAPHANDLER_NOEC(IDE_HANDLER	  , IRQ_OFFSET + IRQ_IDE)
	TRAPHANDLER_NOEC(FIFTEEN_HANDLER  , IRQ_OFFSET + 15)
	TRAPHANDLER_NOEC(ERROR_HANDLER	  , IRQ_OFFSET + IRQ_ERROR)

		SETGATE(idt[IRQ_OFFSET + IRQ_TIMER]    , 0, GD_KT, TIMER_HANDLER	, 0);	
	SETGATE(idt[IRQ_OFFSET + IRQ_KBD]	   , 0, GD_KT, KBD_HANDLER		, 0);
	SETGATE(idt[IRQ_OFFSET + 2]			   , 0, GD_KT, SECOND_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 3]			   , 0, GD_KT, THIRD_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + IRQ_SERIAL]   , 0, GD_KT, SERIAL_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 5]			   , 0, GD_KT, FIFTH_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 6]			   , 0, GD_KT, SIXTH_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + IRQ_SPURIOUS] , 0, GD_KT, SPURIOUS_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 8]			   , 0, GD_KT, EIGHTH_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 9]			   , 0, GD_KT, NINTH_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 10]	   	   , 0, GD_KT, TENTH_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 11]		   , 0, GD_KT, ELEVEN_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 12]		   , 0, GD_KT, TWELVE_HANDLER	, 0);
	SETGATE(idt[IRQ_OFFSET + 13]		   , 0, GD_KT, THIRTEEN_HANDLER , 0);
	SETGATE(idt[IRQ_OFFSET + IRQ_IDE]	   , 0, GD_KT, IDE_HANDLER		, 0);
	SETGATE(idt[IRQ_OFFSET + 15]		   , 0, GD_KT, FIFTEEN_HANDLER  , 0);
	SETGATE(idt[IRQ_OFFSET + IRQ_ERROR]	   , 0, GD_KT, ERROR_HANDLER	, 0);
	这里千万要注意，SETGATE的第二个参数一律都设置为0.因为根据SETGATE的注解：A subsequent IRET instruction restores IF to the value in the EFLAGS image on the stack. 如果设置为1，那么FL_IF每次再iret之后都会被重置，造成bug。

Exercise 14. Modify the kernel's trap_dispatch() function so that it calls
sched_yield() to find and run a different environment whenever a clock interrupt takes place.
You should now be able to get the user/spin test to work: the parent environment should fork off the child, sys_yield() to it a couple times but in each case regain control of the CPU after one time slice, and finally kill the child environment and terminate gracefully.
	最后的exercise是，在时钟中断来临时，调用sched_yield来实现进程切换，引入时间片概念。这里一定记得在调用sched_yield之前需要调用lapic_eoi()来通知lapic我们处理完中断了。

Exercise 15. Implement sys_ipc_recv and sys_ipc_try_send in kern/syscall.c .
Read the comments on both before implementing them, since they have to work together. When you call envid2env in these routines, you should set the checkperm flag to 0, meaning that any environment is allowed to send IPC messages to any other environment, and the kernel does no special permission checking other than verifying that
the target envid is valid. Then implement the ipc_recv and ipc_send functions in lib/ipc.c . Use the user/pingpong and user/primes functions to test your IPC mechanism. user/primes will generate for each prime number a new environment until JOS runs out of environments. You might find it interesting to read user/primes.c to see all the forking and IPC going on behind the scenes.
	首先要实现的是系统调用sys_ipc_recv和sys_ipc_send。
	对于sys_ipc_recv，根据注释，我们需要使用对curenv相关的字段进行修改。首先是env_ipc_recving,将这个字段设置为1，表示当前env处于等待就收message的状态。然后将status设置为ENV_NOT_RUNNABLE,表示不要被cpu调度。并把env_ipc_dstva设置为dstva，这个dstva表示send page需要被映射到的虚拟地址。具体代码如下：
curenv->env_ipc_recving = 1;
curenv->env_ipc_dstva = dstva;
curenv->env_status = ENV_NOT_RUNNABLE;
sched_yield();
return 0;
	对于sys_ipc_send，意义是要发送一个page给另一个env，然后两个env共享当前page。在发完page之后，需要修改dst_env，也就是接受env的状态，具体代码如下：
dst_env->env_ipc_recving = 0;
dst_env->env_ipc_from = curenv->env_id;
dst_env->env_ipc_value = value;
dst_env->env_ipc_perm = srcva == (void *)UTOP ? 0 : perm;
dst_env->env_status = ENV_RUNNABLE;
dst_env->env_tf.tf_regs.reg_eax = 0;
return 0;
	kernel态的系统调用实现完后，接下来就是要实现用户态的相关函数，分别是ipc_recv和ipc_send。
	对于ipc_recv,pg指针指向的就是接受到的page，需要在这个env下被映射到的虚拟地址。From_env_store存储发送的envid，permstore存储接收到的page的权限位。最后返回接受到的value。具体代码如下：
int ret;
if(!pg)
	pg = (void *)UTOP;
ret = sys_ipc_recv(pg);
if(ret < 0){
	if(from_env_store)
		*from_env_store = 0;
	if(perm_store)
		*perm_store = 0;
	return ret;
}
if(from_env_store){
	*from_env_store = getthisenv()->env_ipc_from;
}
if(perm_store){
	*perm_store = getthisenv()->env_ipc_perm;
}
return getthisenv()->env_ipc_value;
	对于ipc_send函数，它有四个参数，to_env表示接收当前发送的消息的env，val表示发送的value，pg表示要发送的page，perm表示发送的pg的权限。由于我们发送这个消息的时候，to_env还没有处于接收信息的状态，因此需要用while循环将发送消息的系统状态包裹起来，然后不断重试。重试的过程中，使用sys_yield()来主动释放cpu，让cpu做更有意义的事情。具体代码如下：
int ret;
if(!pg)
	pg = (void *)UTOP;
while((ret = sys_ipc_try_send(to_env, val, pg, perm))){
	if(ret < 0 && ret != -E_IPC_NOT_RECV){
		panic("panic at ipc_send()\n");
	}
	sys_yield();
}

Challenge! Implement a shared-memory fork() called sfork() . This version should
have the parent and child share all their memory pages (so writes in one environment
appear in the other) except for pages in the stack area, which should be treated in the
usual copy-on-write manner. Modify user/forktree.c to use sfork() instead of
regular fork() . Also, once you have finished implementing IPC in part C, use your
sfork() to run user/pingpongs . You will have to find a new way to provide the
functionality of the global thisenv pointer.
	这个challenge大概就是要修改fork的语义，之前的fork是把父进程的所有有写权限的page都修改成PTE_COW的。这样的话无法实现env共享内存，因为一个env的写另一个env看不到。要做到这一点，只要我们原封不动的把权限映射给子进程，那么他们就可以写同一个虚拟地址空间，彼此都能看见对方。除了ustack部分不能无脑映射，其他映射关系都和父进程相同。具体代码如下：
int ret;
set_pgfault_handler(pgfault);
envid_t child_envid = sys_exofork();
if(child_envid < 0)
	panic("the fork panic! at sys_exofork()\n");
if(child_envid == 0){
	thisenv = &envs[ENVX(sys_getenvid())];
	return 0;
}
for(uintptr_t i = UTEXT; i < USTACKTOP; i+=PGSIZE){	//lab4 bug not TXSTACKTOP
	if(i == (USTACKTOP - PGSIZE))
		duppage(child_envid, PGNUM(i));
	else if((uvpd[PDX(i)] & PTE_P) && ((uvpt[PGNUM(i)] & (PTE_P | PTE_U)) == (PTE_P | PTE_U))){
		if(sys_page_map(0, (void *)(PGNUM(i) * PGSIZE), child_envid, (void *)(PGNUM(i) * PGSIZE), ((uvpt[PGNUM(i)] & (PTE_P | PTE_U | PTE_W)))))
			panic("sys_page_map() panic\n");
	}
}
	
ret = sys_page_alloc(child_envid, (void *)(UXSTACKTOP - PGSIZE), PTE_P | PTE_U | PTE_W);
	if(ret < 0)
		panic("panic in sys_page_alloc()\n");
	ret = sys_env_set_pgfault_upcall(child_envid, _pgfault_upcall);
	if(ret < 0)
		panic("panic in sys_env_set_pgfault_upcall()\n");
	ret = sys_env_set_status(child_envid, ENV_RUNNABLE);
	if(ret < 0)
		panic("panic in sys_env_set_status()\n");
	return child_envid;
	文档提示说，要修改当前thisenv的表示方式，因为之前的thisenv是在无脑拷贝的地址区域内的，因此父子进程共享同一个thisenv，导致它们指向同一个env，所以会出问题，因此我们修改thisenv的表示方法：
extern const volatile struct Env envs[NENV];
const volatile inline struct Env* getthisenv(){
        return &envs[ENVX(sys_getenvid())];
}
	Sys_getenvid通过thiscpu来得到当前的env，而thiscpu返回的envid值就是父子进程对应的envid值，不会出错。修改pingpongs和ipc_recv中的thisenv表示方式，即可通过pingpongs的测试。
